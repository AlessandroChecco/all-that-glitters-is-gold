{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from simhash import Simhash, SimhashIndex\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn import mixture\n",
    "import scipy.stats as stats\n",
    "import pickle\n",
    "import os\n",
    "from scipy.optimize import curve_fit\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import colors\n",
    "def shift_value(rgb, shift):\n",
    "    hsv = colors.rgb_to_hsv(rgb)\n",
    "    hsv[-1] += shift\n",
    "    return colors.hsv_to_rgb(hsv)\n",
    "def color_palette(n_colors):\n",
    "    orig_palette = sns.color_palette(n_colors=n_colors)\n",
    "    shifts = np.linspace(-.3, .3, n_colors)\n",
    "    alternate_shifts = shifts.copy()\n",
    "    alternate_shifts[::2] = shifts[:len(shifts[::2])]\n",
    "    alternate_shifts[1::2] = shifts[len(shifts[::2]):]\n",
    "    palette = [shift_value(col, shift)\n",
    "               for col, shift in zip(orig_palette, alternate_shifts)]\n",
    "    return palette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo\n",
    "- Try again with two instead of three\n",
    "- playing with small f.prob_gold to signal mixed cases\n",
    "- Try to use Bayesian Gaussian Mixture models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "def set_style():\n",
    "    # This sets reasonable defaults for font size for\n",
    "    # a figure that will go in a paper\n",
    "    sns.set_context(\"paper\")\n",
    "    \n",
    "    # Set the font to be serif, rather than sans\n",
    "    sns.set(font='serif')\n",
    "    \n",
    "    # Make the background white, and specify the\n",
    "    # specific font family\n",
    "    sns.set_style(\"whitegrid\", {\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.serif\": [\"Times\", \"Palatino\", \"serif\"]\n",
    "    })\n",
    "def set_size(fig,w=6,h=3):\n",
    "    fig.set_size_inches(w, h)\n",
    "    plt.tight_layout()\n",
    "set_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./political/CSTA-APSR-master/Data - CF jobs/CFjobresults/f269506_html.csv\",low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulator(df,test_quiz=True,steps=-1,gold_per_quiz=8,unit_per_page=10,gold_per_page=1,to_remove=0,verbose=0,gold_options=3):\n",
    "    from IPython import display\n",
    "    def sort_df(df):\n",
    "        return df.sort_values(by='_started_at',ascending=True).reset_index(drop = True)\n",
    "    \n",
    "    def compute_min_ratio(non_golds,test_quiz):\n",
    "        if test_quiz:\n",
    "            return (gold_per_quiz+gold_per_page)/len(non_golds) #max one page \n",
    "        else:\n",
    "            return 1/len(non_golds)\n",
    "   \n",
    "    def remove_first_page(df):\n",
    "        def first_page_group(group):\n",
    "            ll = list(group.groupby('_started_at'))\n",
    "            if len(ll)>1:\n",
    "                return pd.concat([x[1] for x in ll[1:]])\n",
    "            else:\n",
    "                return None\n",
    "        return df.groupby('_worker_id').apply(first_page_group)\n",
    "    \n",
    "    def remove_some(group):\n",
    "        if group.head(1)['_golden'].item() == False:\n",
    "            return group.head(10-to_remove)\n",
    "        else:\n",
    "            return group    \n",
    "    \n",
    "    def clustering_old(data): #data should be np.array shape=(N,1)\n",
    "        gmm = mixture.GaussianMixture(n_components=2,covariance_type='spherical',n_init=10,warm_start=True,means_init=[[1],[10],[20]])\n",
    "        if data.size < 10:\n",
    "            return (np.full(data.size,False),np.full(data.size,np.nan),np.array([np.nan]),gmm)\n",
    "        gmm.fit(data)\n",
    "        if gmm.converged_==False:\n",
    "            print(\"error\",gmm.means_)\n",
    "        predict = gmm.predict(data)\n",
    "        probs= gmm.predict_proba(data)\n",
    "        gold = np.argmax(gmm.means_)\n",
    "        non_gold = np.argmin(gmm.means_)\n",
    "        if (gold != non_gold):\n",
    "            probs = np.array([ t[gold] for t in probs])\n",
    "        else:\n",
    "            probs = np.full(predict.shape,np.nan)\n",
    "            predict_gold = np.full(predict.shape,False)\n",
    "            return (predict_gold,probs,np.array([np.nan]),gmm)\n",
    "        predict_gold=np.full(predict.shape, False)\n",
    "        predict_gold[predict==gold] = True\n",
    "        threshold = find_intercept(gmm.means_[0][0],gmm.means_[1][0],gmm.covariances_[0],gmm.covariances_[1])\n",
    "        if len(threshold) < 1:\n",
    "            threshold = np.array([np.nan])\n",
    "        return (predict_gold,probs,threshold[0],gmm)    \n",
    "    \n",
    "    def clustering(data): #data should be np.array shape=(N,1)\n",
    "        #gmm = mixture.GaussianMixture(n_components=3,covariance_type='spherical',n_init=10,warm_start=True,means_init=[[1],[10],[20]])\n",
    "        gmm = mixture.GaussianMixture(n_components=2,covariance_type='spherical')#,n_init=10,warm_start=True,means_init=[[1],[10],[20]])\n",
    "        if data.size < 10:\n",
    "            return (np.full(data.size,False),np.full(data.size,np.nan),np.array([np.nan]),gmm)\n",
    "        gmm.fit(data)\n",
    "        if gmm.converged_==False:\n",
    "            print(\"error\",gmm.means_)\n",
    "        predict = gmm.predict(data)\n",
    "        probs= gmm.predict_proba(data)\n",
    "        gold = np.argsort(gmm.means_,axis=0)[-1][0]\n",
    "        non_gold = np.argsort(gmm.means_,axis=0)[-2][0]\n",
    "        if (gold != non_gold):\n",
    "            probs = np.array([ t[gold] for t in probs])\n",
    "        else:\n",
    "            probs = np.full(predict.shape,np.nan)\n",
    "            predict_gold = np.full(predict.shape,False)\n",
    "#            print('|Problem'+str(gold)+'-'+str(non_gold)+'-'+str(gmm.means_)+\"t:\"+str([np.nan])+\" |\",end='')\n",
    "            return (predict_gold,probs,np.array([np.nan]),gmm)\n",
    "        predict_gold=np.full(predict.shape, False)\n",
    "        predict_gold[predict==gold] = True\n",
    "        predict_gold[predict!=gold] = predict[predict!=gold]\n",
    "        threshold = find_intercept(gmm.means_[gold][0],gmm.means_[non_gold][0],gmm.covariances_[gold],gmm.covariances_[non_gold])\n",
    "        if threshold[0]<= gmm.means_[gold][0] and threshold[0]>= gmm.means_[non_gold][0]:\n",
    "            correct_threshold = threshold[0]\n",
    "        elif threshold[1]<= gmm.means_[gold][0] and threshold[1]>= gmm.means_[non_gold][0]:\n",
    "            correct_threshold = threshold[1]\n",
    "        else:\n",
    "            correct_threshold = gmm.means_[non_gold][0] + (gmm.means_[gold][0] - gmm.means_[non_gold][0]) /2\n",
    "#        print(str('|')+str(gold)+'-'+str(non_gold)+'-'+str(gmm.means_)+\"t:\"+str(threshold)+\" |\",end='')\n",
    "        if len(threshold) < 1:\n",
    "            threshold = np.array([np.nan])\n",
    "        return (predict_gold,probs,correct_threshold,gmm)    \n",
    "\n",
    "    def clustering_log(data):\n",
    "        def gauss_function(x, a, x0, sigma,b,x1,sigma1):\n",
    "            return a*np.exp(-(x-x0)**2/(2*sigma**2)) + b*np.exp(-(x-x1)**2/(2*sigma1**2))\n",
    "        histy,histx = np.histogram(data)\n",
    "        histy = np.log(histy).clip(0,np.inf)\n",
    "        popt, pcov = curve_fit(gauss_function, histx, histy, p0 = [1, 1, 0.1,0.5,10,10])\n",
    "        threshold = find_intercept(popt[1],popt[4],popt[2],potp[5])\n",
    "        goldvalue = np.max([popt[1],popt[4]])\n",
    "        nongoldvalue = np.min([popt[1],popt[4]])\n",
    "        if threshold[0]<= goldvalue and threshold[0]>= nongoldvalue:\n",
    "            correct_threshold = threshold[0]\n",
    "        elif threshold[1]<= goldvalue and threshold[0]>= nongoldvalue:\n",
    "            correct_threshold = threshold[1]\n",
    "        else:\n",
    "            correct_threshold = nongoldvalue + (goldvalue - nongoldvalue) /2\n",
    "        predict_gold = np.full(data.shape,False)\n",
    "        predict_gold[data>threshold] = True\n",
    "        probs = np.full(data.shape,0.5)\n",
    "        return (predict_gold,probs,correct_threshold,None)\n",
    "        \n",
    "    \n",
    "    def clustering_ok(data): #data should be np.array shape=(N,1)\n",
    "        gmm = mixture.GaussianMixture(n_components=1,covariance_type='spherical')#,n_init=10,warm_start=True,means_init=[[10],[20]])\n",
    "        if data.size < 10:\n",
    "            return (np.full(data.size,False),np.full(data.size,np.nan),np.array([np.nan]),gmm)\n",
    "        gmm.fit(data)\n",
    "        data.shape=(-1,)\n",
    "        value = np.max(gmm.means_)\n",
    "        predict_gold = np.full(len(data),fill_value=False)\n",
    "        probs = np.full(len(data),fill_value=0)\n",
    "        predict_gold[np.where(data>value)] = True\n",
    "        probs[np.where(data>value)] = 1\n",
    "        return (predict_gold,probs,value,gmm)\n",
    "    \n",
    "    def find_intercept(m1,m2,var1,var2):\n",
    "      a = 1/(2*var1) - 1/(2*var2)\n",
    "      b = m2/(var2) - m1/(var1)\n",
    "      c = m1**2 /(2*var1) - m2**2 / (2*var2) - np.log(var2/var1)/2\n",
    "      return np.roots([a,b,c])\n",
    "    \n",
    "    def simulate_work(df):\n",
    "        df['hash'] = df.html.apply(lambda x: Simhash(shingle(x)))\n",
    "        df['multiplicity'] = 0\n",
    "        grouped = df.groupby(['_started_at','_worker_id'],as_index=False,group_keys=False)\n",
    "        for (time,worker),group in grouped:\n",
    "            report_page(group)\n",
    "            for row in group.itertuples(index=True):\n",
    "                idx = row[0]\n",
    "                df.loc[idx,'multiplicity'] = multiplicity[getattr(row,'hash').value]\n",
    "            p_gold,p_prob,thresh = predict_page(group)\n",
    "            df.loc[group.index,'prediction_gold'] = p_gold\n",
    "            df.loc[group.index,'prob_gold'] = p_prob\n",
    "            df.loc[group.index,'threshold'] = thresh\n",
    "        return df\n",
    "\n",
    "    def predict_page(df):\n",
    "        nonlocal counter\n",
    "        idx = np.array(list(multiplicity.keys()),dtype=uint64)\n",
    "        data = np.array(list(multiplicity.values()),dtype=int)\n",
    "        data.shape = (-1,1)\n",
    "        idx.shape = (-1,1)\n",
    "        (predict_gold,probs,threshold,gmm) = clustering(data)\n",
    "        predictor = dict(zip(idx.flatten(),predict_gold))\n",
    "        prob_predictor = dict(zip(idx.flatten(),probs))\n",
    "        p_gold = df.hash.apply(lambda x: x.value).map(predictor)\n",
    "        p_prob = df.hash.apply(lambda x: x.value).map(prob_predictor)\n",
    "        if verbose>0 and (((counter-1) % 100) == 0):\n",
    "            print(threshold,gmm.means_,gmm.covariances_,np.argsort(gmm.means_,axis=0)[-2][0],np.argsort(gmm.means_,axis=0)[-1][0])\n",
    "        return p_gold,p_prob,threshold\n",
    "\n",
    "    def report_page(df): #df is only a page\n",
    "        nonlocal counter\n",
    "        for h in df['hash']:\n",
    "            if h.value in multiplicity:\n",
    "                multiplicity[h.value] += 1\n",
    "            else:\n",
    "                multiplicity[h.value] = 1\n",
    "        if verbose>0 and (counter % 100 == 0):\n",
    "            plt.hist(list(multiplicity.values()))\n",
    "            display.display(plt.gcf())\n",
    "            plt.close()\n",
    "        counter +=1\n",
    "\n",
    "                \n",
    "    def bit_difference(string1, string2):\n",
    "        return Simhash(shingle(string1)).distance(Simhash(shingle(string2)))\n",
    "\n",
    "    def shingle(tokens,length=3):\n",
    "        return [tokens[i:i+length] for i in range(len(tokens) - length + 1) if len(tokens[i]) < 4]\n",
    "    \n",
    "    def worker_subset(df,golds,test_quiz=True): #golds are unique golds, df is worker group\n",
    "        # call it like df.groupby('_worker_id',as_index=False,group_keys=False).apply(lambda x: worker_subset(x,golds ))\n",
    "        df._created_at = pd.to_datetime(df._created_at)\n",
    "        df = df.sort_values(by='_started_at',ascending=True).reset_index(drop = True)\n",
    "        if test_quiz:\n",
    "            assert len(golds)>gold_per_quiz, len(golds)\n",
    "        n_gold_needed = len(df[df._golden==True])\n",
    "        if len(golds) < n_gold_needed:\n",
    "            df = df[((df._golden==True).cumsum()<=len(golds)) | (df._golden==False)]\n",
    "            n_gold_needed = len(golds)\n",
    "        gold_created = golds.sample(n=n_gold_needed,replace=False)\n",
    "        df.loc[df._golden==True,~df.columns.isin(['_started_at','_created_at','_worker_id','_missed'])] \\\n",
    "              = gold_created.loc[:,~gold_created.columns.isin(['_started_at','_created_at','_worker_id','_missed'])].as_matrix()\n",
    "        df._golden = df._golden.astype(bool) #fix case in which inferred type is wrong because no gold present\n",
    "        return df\n",
    "\n",
    "    def compute_accuracy(workgroup,what):\n",
    "        ll = len(workgroup)\n",
    "        golds_first = gold_per_quiz\n",
    "        to_divide = np.insert(np.ones(ll-1),0,golds_first) #add an 8 at the beginning (number of golds)\n",
    "        return (to_divide-workgroup[what]).cumsum()/to_divide.cumsum()\n",
    "    \n",
    "    def compute_page(df):\n",
    "        def FP(row):\n",
    "            tp=tn=fp=fn = 0\n",
    "            if row['_golden']:\n",
    "                if row['prediction_gold']:\n",
    "                    tp = 1\n",
    "                else:\n",
    "                    fn = 1\n",
    "            else:\n",
    "                if row['prediction_gold']:\n",
    "                    fp = 1\n",
    "                else:\n",
    "                    tn = 1\n",
    "            return pd.Series([tp,tn,fp,fn])\n",
    "        df[['tp','tn','fp','fn']]= df.apply(FP,axis=1)\n",
    "        tp = df['tp'].sum(); tn = df['tn'].sum(); fp = df['fp'].sum(); fn = df['fn'].sum()\n",
    "        time_spent=normal_time = df._created_at.iloc[0] -df._started_at.iloc[0]\n",
    "        missed_orig = df['_missed'].sum()\n",
    "        missed = missed_orig\n",
    "        if df['_golden'].sum()<2: #not in first page\n",
    "            if (tp>0 or fp>0): #if I get a signal I ignore the other questions\n",
    "                time_spent = time_spent/10 * (fp+tp)\n",
    "            if (fn==1):\n",
    "                if (missed_orig==0): # if the signal was wrong and I was a good worker I become bad worker with probability\n",
    "                    #missed = 1\n",
    "                    missed = 0 if np.random.randint(0,gold_options)==0 else 1\n",
    "        count = len(df)\n",
    "        \n",
    "        return pd.Series({\"tp\":tp,\"tn\": tn,\"fp\":fp,\"fn\":fn,\"missed\":missed, 'time_spent':time_spent,'count':count,'time_orig':normal_time,'missed_orig':missed_orig})\n",
    "\n",
    "    # START HERE\n",
    "    #clustering = clustering_log\n",
    "    df._missed = df['_missed'].fillna(False)\n",
    "    df._started_at = pd.to_datetime(df._started_at)\n",
    "    df._created_at = pd.to_datetime(df._created_at)\n",
    "    if to_remove > 0:\n",
    "        df = df.groupby('_unit_id',group_keys=False,as_index=False).apply(remove_some)\n",
    "    df = sort_df(df)\n",
    "    if not test_quiz:\n",
    "        print(\"Removing gold \",len(df))\n",
    "        df = sort_df(remove_first_page(df))\n",
    "        print(\"Removed \",len(df))\n",
    "    golds = df[df._golden==True].groupby('_unit_id',as_index=False).first() #unique golds\n",
    "    assert len(golds)>0, \"Dataset is too small\"\n",
    "    non_golds = df[df._golden==False].groupby('_unit_id',as_index=False).first() #unique non-gold\n",
    "    max_ratio = len(golds)/len(non_golds)\n",
    "    min_ratio = compute_min_ratio(non_golds,test_quiz=test_quiz)\n",
    "    if steps == -1:\n",
    "        ratios = 1\n",
    "    else:\n",
    "        ratios = np.linspace(min_ratio,max_ratio,steps)\n",
    "    results = []\n",
    "    performances = []\n",
    "    mults = []\n",
    "    for r in ratios:\n",
    "#        global multiplicity\n",
    "        multiplicity = {}\n",
    "        counter = 1\n",
    "        g = max (1,np.ceil(len(non_golds) * r).astype(int))# number of golds used here\n",
    "        print('Using '+str(g)+' golds('+str(100*r)+\"%)\")\n",
    "        result = sort_df(df.groupby('_worker_id',as_index=False,group_keys=False).apply(lambda x: worker_subset(x,golds.head(g),test_quiz=test_quiz )))\n",
    "        result = simulate_work(result)\n",
    "        performance = sort_df(result.groupby(['_started_at','_worker_id']).apply(lambda group: compute_page(group)).reset_index())\n",
    "        performance['time_cumsum_per_worker'] = performance.groupby('_worker_id',as_index=False,group_keys=False).apply(lambda group: cumsum(group['time_spent']))\n",
    "        performance['count_cumsum'] = performance['count'].cumsum()\n",
    "        performance['bin'] = pd.cut(performance['count_cumsum'],linspace(0,performance['count'].sum(),11),labels=[str((x+1)*10)+'%' for x in range(10)])\n",
    "        performance['time_orig_s'] = performance['time_orig'].apply(lambda x: x.total_seconds())\n",
    "        performance['time_spent_s'] = performance['time_spent'].apply(lambda x: x.total_seconds())\n",
    "        performance['accuracy'] = performance.groupby(['_worker_id'],as_index=False,group_keys=False).apply(lambda x: compute_accuracy(x,'missed'))\n",
    "        performance['accuracy_orig'] = performance.groupby(['_worker_id'],as_index=False,group_keys=False).apply(lambda x: compute_accuracy(x,'missed_orig'))\n",
    "        performances.append(performance)\n",
    "        results.append(result)\n",
    "        mults.append(multiplicity)\n",
    "    final = pd.concat(results, keys=range(len(ratios)))\n",
    "    perf_df = pd.concat(performances, keys=range(len(ratios)))\n",
    "    mult_df = pd.DataFrame.from_records(mults,index=range(len(ratios)))\n",
    "    ratios_s = pd.Series(ratios)\n",
    "    return (final,mult_df,ratios_s,perf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_result(test=True,steps=10,to_remove=0):\n",
    "    filename= '2-'+str(to_remove)+'-'+str(steps)+\"-\"+str(test)+'-'\n",
    "    df_name = filename+'df'\n",
    "    mult_name = filename+'mult'\n",
    "    ratios_name = filename+'ratios'\n",
    "    perf_name = filename+'perf'\n",
    "    f = pd.read_pickle(df_name+'.pkl')\n",
    "    m = pd.read_pickle(mult_name+'.pkl')\n",
    "    r = pd.read_pickle(ratios_name+'.pkl')\n",
    "    p = pd.read_pickle(perf_name+'.pkl')\n",
    "    return f,m,r,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for to_remove in [6,0]:#[4,2,0]:\n",
    "    for test in [True,False]:\n",
    "        steps = 4\n",
    "        filename= '2-'+str(to_remove)+'-'+str(steps)+\"-\"+str(test)+'-'\n",
    "        df_name = filename+'df'+'.pkl'\n",
    "        mult_name = filename+'mult'+'.pkl'\n",
    "        ratios_name = filename+'ratios'+'.pkl'\n",
    "        perf_name = filename+'perf'+'.pkl'\n",
    "        if not os.path.exists(df_name):\n",
    "            f,m,r,p = simulator(df,test_quiz=test,steps=steps,to_remove=to_remove,verbose=0)\n",
    "            f.to_pickle(df_name)\n",
    "            m.to_pickle(mult_name)\n",
    "            r.to_pickle(ratios_name)\n",
    "            p.to_pickle(perf_name)\n",
    "        else:\n",
    "            print(\"Already exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flatui = ['#8dd3c7','#bebada']\n",
    "sns.set_palette(flatui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_perf_time(performance):\n",
    "    melted_performance_time = pd.melt(performance[['time_spent_s', 'time_orig_s', 'bin']].rename(columns={'bin':'Total work progress' ,'time_spent_s':'simhash','time_orig_s':'original'}),value_name='Time [s]' ,var_name=['Technique'],id_vars=['Total work progress'])\n",
    "    sns.pointplot(data=melted_performance_time,x='Total work progress',y='Time [s]',hue='Technique',capsize=0.2,linestyles=['--','-'])\n",
    "    plt.ylabel('Average time per page [s]')\n",
    "    plt.ylim([50,350])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_perf_missed(performance):\n",
    "    workers_per_bin = performance.groupby('bin').apply(lambda group: len(group._worker_id.unique()))\n",
    "    performance['missed'] = performance.apply(lambda x: x.missed/workers_per_bin[x.bin] ,axis=1)\n",
    "    performance['missed_orig'] = performance.apply(lambda x: x.missed_orig/workers_per_bin[x.bin] ,axis=1)\n",
    "    melted_performance_missed = pd.melt(performance[['bin', 'missed','missed_orig']].rename(columns={'bin':'Total work progress' ,'missed':'simhash','missed_orig':'original'}),value_name='Number of gold missed' ,var_name=['Technique'],id_vars=['Total work progress'])\n",
    "    sns.pointplot(data=melted_performance_missed,x='Total work progress',y='Number of gold missed',hue='Technique',capsize=0.2,estimator=sum,linestyles=['--','-'])\n",
    "    plt.ylabel('Average gold missed per worker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_perf_accuracy(performance):\n",
    "    workers_per_bin = performance.groupby('bin').apply(lambda group: len(group._worker_id.unique()))\n",
    "    melted_performance_missed = pd.melt(performance[['bin', 'accuracy','accuracy_orig']].rename(columns={'bin':'Total work progress' ,'accuracy':'simhash','accuracy_orig':'original'}),value_name='Accuracy' ,var_name=['Technique'],id_vars=['Total work progress'])\n",
    "    sns.pointplot(data=melted_performance_missed,x='Total work progress',y='Accuracy',hue='Technique',capsize=0.2,estimator=mean,linestyles=['--','-'])\n",
    "    plt.ylabel('Average accuracy')\n",
    "    plt.ylim([0.7,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that changing the following threshold can help a lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f[(f.prob_gold>0.4) & (f._golden==True)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f[(f.prob_gold<0.1) & (f.prediction_gold == False) & (f._golden==True)].count() #playing with small f.prob_gold to signal mixed cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "steps = 4\n",
    "for test in [True,False]:\n",
    "    for to_remove in [6,0]:#,4,2,0]:\n",
    "        f,m,r,p = load_result(test=test,steps=steps,to_remove=to_remove)\n",
    "        for i in range(steps):\n",
    "            plot_perf_missed(p.loc[i])\n",
    "            plt.title(\"Gold ratio: \"+str(round(100*r.loc[i],2))+\"%, \"+str(10-to_remove)+' judgements per non-gold')\n",
    "            set_size(plt.gcf(),w=6,h=4)\n",
    "            plt.savefig('2-'+str(to_remove)+'-'+str(test)+'-missed-'+str(i)+'.pdf')\n",
    "            plt.close()\n",
    "            plot_perf_time(p.loc[i])\n",
    "            plt.title(\"Gold ratio: \"+str(round(100*r.loc[i],2))+\"%, \"+str(10-to_remove)+' judgements per non-gold')\n",
    "            set_size(plt.gcf(),w=6,h=4)\n",
    "            plt.savefig('2-'+str(to_remove)+'-'+str(test)+'-time-'+str(i)+'.pdf')\n",
    "            plt.close()\n",
    "            plot_perf_accuracy(p.loc[i])\n",
    "            plt.title(\"Gold ratio: \"+str(round(100*r.loc[i],2))+\"%, \"+str(10-to_remove)+' judgements per non-gold')\n",
    "            set_size(plt.gcf(),w=6,h=4)\n",
    "            plt.savefig('2-'+str(to_remove)+'-'+str(test)+'-acc-'+str(i)+'.pdf')\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f.loc[1].threshold[10:].plot()\n",
    "plt.ylim([0,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "temp = f.loc[1]\n",
    "temp[(temp.threshold>15) & (temp._golden==True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def clustering(data): #data should be np.array shape=(N,1)\n",
    "        gmm = mixture.GaussianMixture(n_components=2,covariance_type='spherical',n_init=100,warm_start=True)\n",
    "        if data.size < 10:\n",
    "            return (np.full(data.size,False),np.full(data.size,np.nan),np.array([np.nan]),gmm)\n",
    "        gmm.fit(data)\n",
    "        predict = gmm.predict(data)\n",
    "        probs= gmm.predict_proba(data)\n",
    "        gold = np.argmax(gmm.means_)\n",
    "        non_gold = np.argmin(gmm.means_)\n",
    "        if (gold != non_gold):\n",
    "            probs = np.array([ t[gold] for t in probs])\n",
    "        else:\n",
    "            probs = np.full(predict.shape,np.nan)\n",
    "            predict_gold = np.full(predict.shape,False)\n",
    "            return (predict_gold,probs,np.array([np.nan]),gmm)\n",
    "        predict_gold=np.full(predict.shape, False)\n",
    "        predict_gold[predict==gold] = True\n",
    "        threshold = find_intercept(gmm.means_[0][0],gmm.means_[1][0],gmm.covariances_[0],gmm.covariances_[1])\n",
    "        if len(threshold) < 1:\n",
    "            threshold = np.array([np.nan])\n",
    "        return (predict_gold,probs,threshold[0],gmm)\n",
    "    \n",
    "    def find_intercept(m1,m2,var1,var2):\n",
    "      a = 1/(2*var1) - 1/(2*var2)\n",
    "      b = m2/(var2) - m1/(var1)\n",
    "      c = m1**2 /(2*var1) - m2**2 / (2*var2) - np.log(var2/var1)/2\n",
    "      return np.roots([a,b,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gmm = mixture.GaussianMixture(n_components=2,covariance_type='spherical',n_init=100)\n",
    "gmm.fit(m.loc[2].as_matrix().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict = gmm.predict(m.loc[2].as_matrix().reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gmm.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold = find_intercept(gmm.means_[0][0],gmm.means_[1][0],gmm.covariances_[0],gmm.covariances_[1])\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=1)\n",
    "df.groupby('pred').hist(alpha=0.9,ax=axs,log=True)\n",
    "plt.legend(['False','True'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(mults[1],index=[0]).T\n",
    "df.loc[np.array(list(mults[1].keys()),dtype=uint64),'pred'] = clustering(np.array(list(mults[1].values())).reshape(-1,1))[0]\n",
    "df.rename(columns={0: 'multiplicity'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = df[df.pred==True].multiplicity.as_matrix()\n",
    "y = df[df.pred==False].multiplicity.as_matrix()\n",
    "xweights = 100 * np.ones_like(x) / x.size\n",
    "yweights = 100 * np.ones_like(y) / y.size\n",
    "\n",
    "pyplot.hist(x, weights=xweights, alpha=0.9, label='x')\n",
    "pyplot.hist(y, weights=yweights, alpha=0.9, label='y')\n",
    "pyplot.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(np.array(list(mults[1].values())),log=True);\n",
    "plt.xlabel('cluster multiplicity')\n",
    "plt.ylabel('count')\n",
    "plt.title('6% gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(np.array(list(mults[2].values())),log=True);\n",
    "plt.xlabel('cluster multiplicity')\n",
    "plt.ylabel('count')\n",
    "plt.title('12% gold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#result = pd.read_pickle(\"result.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "percentages = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = result.loc[4]\n",
    "df._missed = df['_missed'].fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normal_time = (df['_created_at']-df['_started_at']).median()\n",
    "quick_time = normal_time/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = pd.DataFrame([[1,2,3],[2,3,4]])\n",
    "def test(df):\n",
    "    return pd.Series([df.mean(),df.max()])\n",
    "a[['c','d']]=a.apply(test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_page(df,quick_time,normal_time):\n",
    "    def FP(row):\n",
    "        tp=tn=fp=fn = 0\n",
    "        if row['_golden']:\n",
    "            if row['prediction_gold']:\n",
    "                tp = 1\n",
    "            else:\n",
    "                fn = 1\n",
    "        else:\n",
    "            if row['prediction_gold']:\n",
    "                fp = 1\n",
    "            else:\n",
    "                tn = 1\n",
    "        return pd.Series([tp,tn,fp,fn])\n",
    "    df[['tp','tn','fp','fn']]= df.apply(FP,axis=1)\n",
    "    tp = df['tp'].sum(); tn = df['tn'].sum(); fp = df['fp'].sum(); fn = df['fn'].sum()\n",
    "    time_spent = normal_time\n",
    "    missed_orig = df['_missed'].sum()\n",
    "    missed = missed_orig\n",
    "    if df['_golden'].sum()<2: #not in first page\n",
    "        if (tp==1 or fp==1): #if I get a signal I ignore the other questions\n",
    "            time_spent = quick_time\n",
    "        if (fn==1):\n",
    "            if (missed_orig==0): # if the signal was wrong and I was a good worker I become bad worker\n",
    "                missed = 1\n",
    "    count = len(df)\n",
    "    return pd.Series({\"tp\":tp,\"tn\": tn,\"fp\":fp,\"fn\":fn,\"missed\":missed, 'time_spent':time_spent,'count':count,'time_orig':normal_time,'missed_orig':missed_orig})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "piece = list(list(df.groupby('_worker_id'))[0][1].groupby('_started_at'))[0][1]\n",
    "piece.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "performance = sort_df(df.groupby(['_started_at','_worker_id']).apply(lambda group: compute_page(group,quick_time,normal_time)).reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "performance['time_cumsum_per_worker'] = performance.groupby('_worker_id',as_index=False,group_keys=False).apply(lambda group: cumsum(group['time_spent']))\n",
    "performance['count_cumsum'] = performance['count'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "performance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "performance['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linspace(0,performance['count'].sum(),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "performance['bin'] = pd.cut(performance['count_cumsum'],linspace(0,performance['count'].sum(),11),labels=[str((x+1)*10)+'%' for x in range(10)])\n",
    "performance['time_orig_s'] = performance['time_orig'].apply(lambda x: x.total_seconds())\n",
    "performance['time_spent_s'] = performance['time_spent'].apply(lambda x: x.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "performance.bin.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "performance.groupby('bin')['missed'].sum(),performance.groupby('bin')['missed_orig'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "performance.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "melted_performance_time = pd.melt(performance[['bin', 'time_orig_s','time_spent_s']].rename(columns={'bin':'Total work progress' ,'time_spent_s':'simhash','time_orig_s':'original'}),value_name='Time [s]' ,var_name=['Technique'],id_vars=['Total work progress'])\n",
    "melted_performance_missed = pd.melt(performance[['bin', 'missed','missed_orig']].rename(columns={'bin':'Total work progress' ,'missed':'simhash','missed_orig':'original'}),value_name='Number of gold missed' ,var_name=['Technique'],id_vars=['Total work progress'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.violinplot(data=performance,x='bin',y='time_spent_s',cut=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "melted_performance_missed = pd.melt(performance[['bin', 'missed','missed_orig']].rename(columns={'bin':'Total work progress' ,'missed':'simhash','missed_orig':'original'}),value_name='Number of gold missed' ,var_name=['Technique'],id_vars=['Total work progress'])\n",
    "melted_performance_time = pd.melt(performance[['time_spent_s', 'time_orig_s', 'bin']].rename(columns={'bin':'Total work progress' ,'time_spent_s':'simhash','time_orig_s':'original'}),value_name='Time [s]' ,var_name=['Technique'],id_vars=['Total work progress'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.violinplot(data=melted_performance_missed,x='Total work progress',y='Number of gold missed',hue='Technique',cut=0,inner=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.violinplot(data=melted_performance_time,x='Total work progress',y='Time [s]',hue='Technique',cut=0,inner=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pointplot(data=melted_performance_time,x='Total work progress',y='Time [s]',hue='Technique',capsize=0.2,)\n",
    "plt.ylabel('Time [s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.pointplot(data=melted_performance_missed,x='Total work progress',y='Number of gold missed',hue='Technique',capsize=0.2,)\n",
    "plt.ylabel('Empirical probability of missing a gold question')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.random.normal(loc=1,scale=1,size=(N,1))\n",
    "b = np.random.normal(loc=10,scale=1,size=(N,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.concatenate([a,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "notify_time": "30",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
